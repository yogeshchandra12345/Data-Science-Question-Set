## Questions
- [Basics of Natural Language Processing ](#Basics-of-Natural-Language-Processing)
- [Sequence Modelling](#Sequence-Modelling)
- [Autoencoders](#Autoencoders)
- [Representation Learning](#Representation-Learning)
- [Monte Carlo Methods](#Monte-Carlo-Methods)

### Basics of Natural Language Processing 

1. What is WORD2VEC? 
2. What is t-SNE? Why do we use PCA instead of t-SNE? 
3. What is sampled softmax? 
4. Why is it difficult to train a RNN with SGD? 
5. How do you tackle the problem of exploding gradients? 
6. What is the problem of vanishing gradients? 
7. How do you tackle the problem of vanishing gradients? 
8. Explain the memory cell of a LSTM. 
9. What type of regularization do one use in LSTM? 
10. What is Beam Search? 
11. How to automatically caption an image? 

### Sequence Modelling
1. Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN? 
2. What determines the size of an unfolded graph? 
3. What are the advantages of an unfolded graph? 
4. What does the output of the hidden layer of a RNN at any arbitrary time t represent? 
5. Are the output of hidden layers of RNNs lossless? If not, why? 
6. RNNs are used for various tasks. From a RNNs point of view, what tasks are  more demanding than others? 
7. Discuss some examples of important design patterns of classical RNNs. 
8. Write the equations for a classical RNN where hidden layer has recurrence.  How would you define the loss in this case? What problems you might face while training it? 
9. What is back-propagation through time? 
10. Consider a RNN that has only output to hidden layer recurrence. What are its advantages or disadvantages compared to a RNN having only hidden to hidden recurrence? 
11. What is Teacher forcing? Compare and contrast with BPTT.
12. What is the disadvantage of using a strict teacher forcing technique? How to solve this?
13. Explain the vanishing/exploding gradient phenomenon for recurrent neural networks.
14. Why don’t we see the vanishing/exploding gradient phenomenon in feedforward networks?
15. What is the key difference in architecture of LSTMs/GRUs compared to traditional RNNs? 
16. What is the difference between LSTM and GRU?
17. Explain Gradient Clipping. 
18. Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why? 
19. Discuss RNNs in the context of Bayesian Machine Learning. 
20. Can we do Batch Normalization in RNNs? If not, what is the alternative? 

### Autoencoders 
1. What is an Autoencoder? What does it “auto-encode”? 
2. What were Autoencoders traditionally used for? Why there has been a resurgence of Autoencoders for generative modeling? 
3. What is recirculation? 
4. What loss functions are used for Autoencoders? 
5. What is a linear autoencoder? Can it be optimal (lowest training reconstruction error)? If yes, under what conditions? 
6. What is the difference between Autoencoders and PCA? 
7. What is the impact of the size of the hidden layer in Autoencoders? 
8. What is an undercomplete Autoencoder? Why is it typically used for? 
9. What is a linear Autoencoder? Discuss it’s equivalence with PCA. Which one is better in reconstruction? 
10. What problems might a nonlinear undercomplete Autoencoder face? 
11. What are overcomplete Autoencoders? What problems might they face? Does the scenario change for linear overcomplete autoencoders? 
12. Discuss the importance of regularization in the context of Autoencoders. 
13. Why does generative autoencoders not require regularization? 
14. What are sparse autoencoders? 
15. What is a denoising autoencoder? What are its advantages? How does it solve  the overcomplete problem? 
16. What is score matching? Discuss it’s connections to DAEs. 
17. Are there any connections between Autoencoders and RBMs? 
18. What is manifold learning? How are denoising and contractive autoencoders  equipped to do manifold learning?
19. What is a contractive autoencoder? Discuss its advantages. How does it solve the overcomplete problem? 
20. Why is a contractive autoencoder named so? 
21. What are the practical issues with CAEs? How to tackle them? 
22. What is a stacked autoencoder? What is a deep autoencoder? Compare and contrast. 
23. Compare the reconstruction quality of a deep autoencoder vs. PCA.
24. What is predictive sparse decomposition? 
25. Discuss some applications of Autoencoders. 

### Representation Learning 
1. What is representation learning? Why is it useful? 
2. What is the relation between Representation Learning and Deep Learning? 
3. What is one-shot and zero-shot learning (Google’s NMT)? Give examples. 
4. What trade offs does representation learning have to consider? 
5. What is greedy layer-wise unsupervised pretraining (GLUP)? Why greedy? Why layer-wise? Why unsupervised? Why pretraining?
6. What were/are the purposes of the above technique? (deep learning problem and initialization) 
7. Why does unsupervised pretraining work? 
8. When does unsupervised training work? Under which circumstances? 
9. Why might unsupervised pretraining act as a regularizer? 
10. What is the disadvantage of unsupervised pretraining compared to other forms of unsupervised learning? 
11. How do you control the regularizing effect of unsupervised pretraining? 
12. How to select the hyper-parameters of each stage of GLUP? 

### Monte Carlo Methods 
1. What are deterministic algorithms? 
2. What are Las vegas algorithms? 
3. What are deterministic approximate algorithms? 
4. What are Monte Carlo algorithms? 
